# -*- coding: utf-8 -*-
"""MLworkspace.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VS_VVA4xmw-gpXDqaaGlWu6SdU1-I9JC

# **Data Preprocessing Template**

# ***Importing the libraries***
"""

#Swasthika is CODING "Danger Zone" Beware NOOBS!!

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

"""# ***Importing the dataset***"""

# x is the attributes and y is the output
dataset = pd.read_csv('Data.csv')
x = dataset.iloc[:,:-1].values
y =  dataset.iloc[:,-1].values

print(x)

print(y)

"""# ***Taking Care Of Missing Data***"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')
imputer.fit(x[:,1:3])
#fit method to connect here with the table
x[:,1:3] = imputer.transform(x[:,1:3])
#transform to apply transformation

print(x)

"""# ***Encoding Categorial data***

encoding dependent variable
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [0])], remainder='passthrough')
# encoding catergories column (column 0) using the class OneHotEncoder  
#[0] corresponds to 0th column
#have to keep the other columns too/ i.e remainder is equal to passthrough
#z=ct.fit_transform(x)
x=np.array(ct.fit_transform(x))

#fit transform in one but it returns a matrix x as a numpy array , o making it an array 
#used this as there were more columns

print(x)

#print(z)

"""encoding dependent variable"""

from sklearn.preprocessing import LabelEncoder
l = LabelEncoder()
y = l.fit_transform(y)
print(y)

"""# ***Splitting the dataset into the Training set and Test set***"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

print(x_train)

print(x_test)

print(y_train)

print(y_test)

"""# ***Feature Scaling***"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train[:,3:] =sc.fit_transform(x_train[:,3:])
x_test[:,3:] = sc.fit_transform(x_test[:,3:])

print(x_train)

print(x_test)

"""# **Regression**

# ***Simple Linear Regression model***
"""

# step 1 : import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

# step 2 : Get the dataset

dataset = pd.read_csv('Salary_Data.csv')
x = dataset.iloc[:,:-1].values
y =  dataset.iloc[:,-1].values


# step 3 : split the dataset 

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

#training the model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(x_train,y_train)

# predict the results of the test examples x , i.e is we get y predicted
y_pred = regressor.predict(x_test)

#visualizing the training set results 
plt.scatter(x_train,y_train, color = 'black')
plt.plot(x_train, regressor.predict(x_train), color = 'green')
plt.title('Linear regression model (Train set)')
plt.xlabel('years of experience')
plt.ylabel('SALARY')
plt.show()

#visualizing testing set results

plt.scatter(x_test,y_test, color = 'yellow')
plt.plot(x_train, regressor.predict(x_train), color = 'red')
plt.title('Linear regression model(Test set)')
plt.xlabel('years of experience')
plt.ylabel('SALARY')
plt.show()

# making a single prediction 
print(regressor.predict([[12]]))

# getting the linear equation 

print(regressor.intercept_)
print(regressor.coef_)
# eqn is Salary = regressor.intercept_ + regressor.coef_ * Number Of Years

"""# ***Multiple Linear Regression***"""

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('50_Startups.csv')
x = dataset.iloc[:,:-1].values
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#encoding categorial data
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [3])], remainder='passthrough')
x=np.array(ct.fit_transform(x))
print(x)

#Do we have to do something to avoid multivariable trap?? NO
#Sklearn will automatically find the p value / So we dont have to do anything to find the P-Value
#splitting the data set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

# Training the Multi linear Regression Model

#same as linear regression, it will automatically recognize. that it is an multi linear regression as it has many variables
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
#fit method ytains the data on the model chosen
regressor.fit(x_train,y_train)

# predicting the testing set results 
#cant plot the graph as linear regression graph as there are more than 2 variables
#all numpy library functions called by np (the alias)
y_pred = y_pred = regressor.predict(x_test) 
np.set_printoptions(precision=2)
#concatenate is an numpy function takes in tuples you want to concatenate , 2nd argument is axis = 0/1 takes vertical or horizontal concatenation   
#normally when print is used, it is written horizontally. to reshape vector n arrays use reshape function (no of rows , columns)
print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))

"""# ***Polynomial Regression***"""

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Position_Salaries.csv') 

#leave the 1st column
x = dataset.iloc[:,1:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

# This time we have both linear n polynomial regression 
# training the linear regression modle on the whole data set
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
#fit method ytains the data on the model chosen
lin_reg.fit(x,y)

# As we know that these are 

# Polynomial Regression Model
from sklearn.preprocessing import PolynomialFeatures
#Polynomial regression of degree 2 i.e y = b0 + b1 * x^1 + b2 * x^2
poly_reg = PolynomialFeatures(degree = 9) #degree 4 works nearly perfectly
x_poly = poly_reg.fit_transform(x)
lin_reg_2 =   LinearRegression()
lin_reg_2.fit(x_poly,y)

# visualize linear regression model 
plt.scatter(x,y,color = 'pink')
plt.plot(x,lin_reg.predict(x), color = 'blue')
plt.title('Linear Regression')
plt.xlabel('Position / Level')
plt.ylabel('Salary')
plt.show()

#visualize polnomial regression model 
plt.scatter(x,y,color = 'pink')
plt.plot(x,lin_reg_2.predict(x_poly), color = 'blue')
plt.title('Polynomial Regression')
plt.xlabel('Position / Level')
plt.ylabel('Salary')
plt.show()

# prediction using linear regression

lin_reg.predict([[6.5]])

#x_poly = poly_reg.fit_transform(x)
# predction using polynomial regression

lin_reg_2.predict(poly_reg.fit_transform([[6.5]]))

"""# ***SUPPORT VECTOR REGRESSION (SVR)***"""

# Have to do feature scaling as the is a specific equation for SVR
#import library
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Position_Salaries.csv') 

#leave the 1st column
x = dataset.iloc[:,1:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#reshape the 2d array displaying vertically (Why 2d array becasue the standarized Feature scale class expects a 2D array)
#Reshaping vertically, with n rows and 1 column
y = y.reshape(len(y),1) # reshape(no. of rows, no. of colums)
print(y)

#feature scaling - Making the data within a range. (when the values are super high)
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
x  =sc_x.fit_transform(x) #there is no train and test set here 
#note make 2 standardScaler object one to maintain the mean/standard deviation etc of the levels and salary differently
sc_y = StandardScaler()
y =sc_y.fit_transform(y)
print(x)
print(y)
#values ranging from -3 and +3

# Training the SVR model on the whole dataset
#use skit learn (svm module from which svr class) 
from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(x, y)

# regressor = SVR(kernel = 'rbf')  check on SVM kernels (here we are using RBF kernel)
# regressor.fit(x,y) train

#reversing the scaling to get the original salary and not the scale value as the final answer
#sc_y.inverse_transform(regressor.predict(sc_x.transform([[6.5]]))) #gives scaled value
#Applying inverse transformation to go the original value

sc_y.inverse_transform(regressor.predict(sc_x.transform([[6.5]])))

#visualizing the results
plt.scatter(sc_x.inverse_transform(x),sc_y.inverse_transform(y),color='red')
plt.plot(sc_x.inverse_transform(x),sc_y.inverse_transform(regressor.predict(x)),color='green')
plt.title('SVM')
plt.xlabel('Level')
plt.ylabel('Salary')

#visualize SVR curve in higher resolution and smooth curve
x_grid = np.arange(min(sc_x.inverse_transform(x)),max(sc_x.inverse_transform(x)),0.1)
x_grid=x_grid.reshape((len(x_grid),1))
plt.scatter(sc_x.inverse_transform(x),sc_y.inverse_transform(y),color='red')
plt.plot(x_grid,sc_y.inverse_transform(regressor.predict(sc_x.transform(x_grid))),color='green')
plt.title('SVM')
plt.xlabel('Level')
plt.ylabel('Salary')
plt.show()

"""# ***Decision Tree Regression***"""

#Note:  Decision Tree Regression is suitable for multidimensional data as it splits the data, i.e data with many attributes

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Position_Salaries.csv') 

#leave the 1st column
x = dataset.iloc[:,1:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#Training the decision tree regression modle on the whole dataset

#No future scaling required for both decision tree and random forest because they are not some equations, spilts of data
#Taking care of missing data or encoding might be required
#Only visualizing the regression tree in higher resolution

from sklearn.tree import DecisionTreeRegressor #predicts continous numerical value whereas DecisionTreeClassifier predicts a category
regressor = DecisionTreeRegressor(random_state = 0) #random factors are happening which may effect the values
regressor.fit(x,y)

#predicting a new result 
regressor.predict([[6.5]])

#High Resolution Visualization the decision tree regression results
x_grid = np.arange(min(x),max(x),0.1)
x_grid=x_grid.reshape((len(x_grid),1))
plt.scatter(x,y,color='red')
plt.plot(x_grid,regressor.predict(x_grid),color='green')
plt.title(' Decision Tree Regression')
plt.xlabel('Level')
plt.ylabel('Salary')
plt.show()

#Low Resolution Visualization
plt.scatter(x,y,color='red')
plt.plot(x,regressor.predict(x),color='green')
plt.title('Decision Tree Regression')
plt.xlabel('Level')
plt.ylabel('Salary')

#Making Decision Tree Regression model for higher resolution data
# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Position_Salaries.csv') 

#leave the 1st column
x = dataset.iloc[:,1:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values #These may vary depending on the data
print(x)
print(y)
#use missing value tool
#Any categorical data -  Apply Encoding,
#Can split data
#no future scaling

from sklearn.tree import DecisionTreeRegressor #predicts continous numerical value whereas DecisionTreeClassifier predicts a category
regressor = DecisionTreeRegressor(random_state = 0) #random factors are happening which may effect the values
regressor.fit(x,y)

#predicting a new result 
regressor.predict([[6.5]]) 
#dataset with mutiple features    regressor.predict([[a,b,c]])

"""# ***Random Forest Regression***"""

#Random forest is a version of Essemble learning(Essemble learning - Take same or different algorithms multiple times and create something much more powerful)
#step1 : pick a random k data points from the training set
#step2 : Build a decision tree based on these k data points
#step 3: choose the number of N trees you want to build in steps 1 and step 2
#For a new data point, make each one of your N tree predict the value of Y to for the data point in question, and assign the new data point the average across all of the predicted Y values

#import library
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Position_Salaries.csv') 

#leave the 1st column
x = dataset.iloc[:,1:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#Training the Random Forest Regression Model on the whole dataset
from sklearn.ensemble import RandomForestRegressor 
regressor = RandomForestRegressor(n_estimators = 10 , random_state = 0) #very important parameter
#number of trees is given here given as n_estimators = 10 
regressor.fit(x,y) #Basically training
regressor.predict([[6.5]])

#Visualizing the Random Forest Regression results (High resolution)
x_grid = np.arange(min(x),max(x),0.1)
x_grid=x_grid.reshape((len(x_grid),1))
plt.scatter(x,y,color='red')
plt.plot(x_grid,regressor.predict(x_grid),color='green')
plt.title(' Random Forest Regression')
plt.xlabel('Level')
plt.ylabel('Salary')
plt.show()

"""# ***R Squared Intuition and Adjusted R^2***"""

#Check a youtube video for better understanding

#R squared Intuition 


#(Sum of Squares) SSres = SUM (yi - yi^)2 
# SStotal = SStot = SUM(yi - yavg)2
#minimize sum of the squares, i.e os trying to fit the best fitting line.
#R^2 = 1 - SSres/SStot (R square can be negative, i.e model is broken. closer its to 1 its better)
#Basically tells us how good our model is fitted to the data
#Bigger the R^2 the better i.e closer it is to 1, its better

# Adjusted R^2

#Now the problem arrives when the number of parameters are more, With the 3rd vairable did the x become better ?? 
#R^2 never decrease, only increases 
# Adj R^2 = 1 - (1-R^2)(n-11/n-p-1)
# p - number of regressor , n - Sample size

"""# ***Which Regression Model to use tool on any given data set***"""

#Making a generic template that you can quickly compare the best regression model
#checking the precision and accuracy
#try all the models and the performance is measured by co-efficient if R^2 (Compare using R^2)
# sklearn.metrics use this to find the best one

# from sklearn.metrics import r2_score
# r2_score(y_test,y_pred)

"""# **CLASSIFICATION**

# ***Logistic Regression Model***
"""

#Think of it as a probability of an event happening

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)

#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

print(x_test)

#Training the logistic model on the training dataset

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)
#in the output you can see c the smaller the value of c the better 

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

#similar to multiple linear regression
# predicting the testing set results 
y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#65 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#24 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#3 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#8 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#89 correct entries 65 + 24 = 89 in terms of 100

#code is very advanced and might not use it often might be never!
#visualizing training set
#green region is where they bought n red where it wasnt bought 
#same color as region is an example of correct prediction
#the prediction boundary, is a straight line as its linear classifier, but for non linear model it wont be a straight line.
#for better prediction the prediction boundary shouln't be a straight line, optimally a prediction curve, which we will get in non linear classification
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_test), y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# #Check on udemy for free course named - Logistic Regression Practical Case Study :)

# ***KNN ALGORITHM***
"""

#step by step rule for knn
#choose the no of neighbors (K) default value is 5
#take the knn data points according to their euclidian distance can use other distances also manhatten distance etc
#count no of data point in each category 
#assign new data points to the neighbours with most neighbours 
#euclidian distance d = root((x1-x2)^2 + (y1 - y2)^2)

#step is same as logistic regression, just while testing we need to change the classifier

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

print(x_test)

#Training the knn model on the training dataset
#uniform value of weight is default
#algo = auto, fits best   
#uses classic euclidian distance

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)
classifier.fit(x_train, y_train)
#in the output you can see c the smaller the value of c the better 

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

# predicting the testing set results 
y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#64 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#29 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#4 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#3 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#93 correct entries 64 + 29 = 93 in terms of 100

#code is very advanced and might not use it often might be never!
#visualizing training set
#from sklearn.preprocessing import StandardScaler
#sc = StandardScaler()
#takes time, Note the model is not having a linear line but cursive covering more data points than logistic regression model 
#you mightnot be able to run it on COLLAB as it needs more RAM 
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('KNN  (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('K-NN (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Support Vector Machine (SVM)***"""

#finding the best line that seperates, so we find maximum margin which gives us the line which is equidistant from 2 lines
#the 2 lines formed are called support vector points
#most general word is vector 
#step by step rule for knn
#choose the no of neighbors (K) default value is 5
#take the knn data points according to their euclidian distance can use other distances also manhatten distance etc
#count no of data point in each category 
#assign new data points to the neighbours with most neighbours 
#euclidian distance d = root((x1-x2)^2 + (y1 - y2)^2)

#step is same as logistic regression, just while testing we need to change the classifier

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

print(x_test)

#Training the logistic model on the training dataset
# kernel trick, a method of using a linear classifier to solve a non-linear problem.
#The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable.
#can choose both linearsvm or kernal one
#INPUT TAKES IN KERNAL i.e either linear or a non linear kernal
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(x_train, y_train)

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

# predicting the testing set results 
y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#66 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#24 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#2 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#8 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#90 correct entries 66 + 24 = 90 in terms of 100

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Kernel SVM MODEL***"""

#Non linearly seperable data
#but the machine assumes its linearly seperable
#helps us choose optimal decision boundary, so we have to develop an algo for this case in which we can't draw a line and seperate these
#learn how to map data to a higher dimension, Apply some kind of mapping function
#the kernal trick- check on gausian kernal function
#for more complex use 2 complex function. 
#types of kernel functions -

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(x_train, y_train)
#in the output you can see c the smaller the value of c the better 

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#64 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#29 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#4 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#3 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#93 correct entries 64 + 29 = 90 in terms of 100

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Naive Bayes***"""

#its called naive because these theorems include individual assumptions, which are often not correct. 
#Bayes theorem - Bayes' theorem states that the conditional probability of an event, X, given the occurrence of another event, Y, is equal to the product of the likelihood of Y given X and the probability of X 
#formula - P ( happens | X) = P ( X âˆ£ happens ) P ( Happens )/ P ( X ) .
#here we compare two probailities of an event happening and not happening, we can see that the denominator will be same and can ignore it
#naives bayes is probabilistic class gives posterior probability,
#Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. 
#a marginal likelihood function, or integrated likelihood, is a likelihood function in which some parameter variables have been marginalized.

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(x_train, y_train)

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#65 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#25 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#3 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#7 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#93 correct entries 65 + 25 = 90 in terms of 100

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Decision Tree***"""

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(x_train, y_train)

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#62 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#29 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#6 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#3 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#91 correct entries 62 + 29 = 91 in terms of 100

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Random Forest Classification***"""

#what's ensemble learning -
#random forest uses many decision tree's
#steps 
#pick a random k data point on the data set
#build decision for some of the data set
#choose n number of trees from 1st 2 steps
#make each datapoint predict the category to which the data belongs to majority of the prediction output

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

print(x_train)

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10 ,criterion = 'entropy', random_state = 0)
classifier.fit(x_train, y_train)

#predicting new result
print(classifier.predict(sc.transform([[30,87000]])))

y_pred = classifier.predict(x_test)
#as its 0 or 1 we need not need precision
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

# making the confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

#finding accuracy
accuracy_score(y_test,y_pred)

#63 correct prediction of class 0 meaning customer of the test set who didnt buy the SUV 
#28 correct prediction of the class 1  meaning customer of the test set who bought the SUV 
#5 incorrect prediction of class 1 meaning the customer who actually bought but were predicted not to
#4 incorrect prediction of class 0 meaning the customer who didnt actually bought but were predicted they had got it

#91 correct entries 63 + 28 = 91 in terms of 100

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Best Model / Max Efficiency***"""

#use the following template to analyse breast cancer data to find the best classification
#decision tree wins for breast cancer example
#things to know about
#false positive - 
#false negative - 
#confusion matrix - (actual, predicted)
#accuracy paradox - 
#CAP curve -(Cummulative accuracy profile)
#CAP curve analysis
#receiver operating characteristics

"""# **Clustering**

# ***K- Means Clustering***
"""

#clustering is basically identifying patterns in data
#steps

#1 Select the number of clusters k
#2 random k points that can be centroid of the points (not really the data points)
#3 assign each datapoint to the closest centroid - that form k cluster
#4 compute n place each new centroid
#5 reassign the data point to new closest centroid, if assignment takes place den go to step 4. else finalize te cluster

# your model is ready

# Go through k means ++ algorithm

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Mall_Customers.csv') 

#Check the dataset, here last column can be used in understandong the data
x = dataset.iloc[:,[3,4]].values  #customer id isnt needed as its a random no 
# 3,4 becasue during visualization the final graph will be better in 2D, and as the last 2 columns have higher levels of sense when it comes to spedning
print(x)

#use the elbow method to find the number of clusters
from sklearn.cluster import KMeans
#not class of KMeans directly
wcss = []
for i in range(1 ,11): # includes 1 to 10 , 11 is excluded
  kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
  kmeans.fit(x)
  wcss.append(kmeans.inertia_) #basically trying to minimiza the distance
plt.plot(range(1 ,11), wcss)
plt.title('The Elbow Method')
plt.xlabel('no of clusters')
plt.ylabel('wcss')
plt.show()

#training the k means model on the dataset
#from the graph we observe that the graph decreases slowly after the number of cluster is = 5, which means 5 is the optimum number of clusters needed
kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(x) #fits  and predicts and returns the value
print(y_kmeans)
# 5 clusters 0 - 4 , 1st customer belongs to cluster 3

#visualizing the clusters
plt.scatter(x[y_kmeans == 0,0],x[y_kmeans == 0,1], s= 100, c = 'red', label = 'Cluster 1') #scattering cluster of index 0 , y_kmeans == 0 means all the ones corresponding to the cluster 0 and plot against income and next plost against purchase course
#(x,y) = (cluster 0 with income , cluster 0 with spending score), s = size, 
plt.scatter(x[y_kmeans == 1,0],x[y_kmeans == 1,1], s= 100, c = 'blue', label = 'Cluster 2') 
plt.scatter(x[y_kmeans == 2,0],x[y_kmeans == 2,1], s= 100, c = 'green', label = 'Cluster 3') 
plt.scatter(x[y_kmeans == 3,0],x[y_kmeans == 3,1], s= 100, c = 'pink', label = 'Cluster 4') 
plt.scatter(x[y_kmeans == 4,0],x[y_kmeans == 4,1], s= 100, c = 'yellow', label = 'Cluster 5') 

#pasting the centroid of the clusters

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'magenta', label = 'Centroids')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend() #for the chart at the right telling you which color belongs to what
plt.show()

"""# ***Hierarchial Clustering***"""

# 2 types 
#1 agglometric (make each data point as a single cluster, nearest data point, take 2 data points make them n-1 clusters, take 2 closest cluster and make n-2 cluster, repeat till there's one cluster) 
#2 and Divisive
#learn about dendrogram and its construction
# ways to calculate distance effects the ooutput of our model
#closest point distance
#furtherest
#average
#centroid

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Mall_Customers.csv') 

x = dataset.iloc[:,[3,4]].values
print(x)

#build a dendogram model
import scipy.cluster.hierarchy as sch # same as from import ..
dendrogram = sch.dendrogram(sch.linkage(x,method = 'ward')) #getting the dendogram function , ward used to minimize variance check, about it on wiki
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

#Training the Hierarchieal model on the dataset
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward') # n_clusters was 5 according to dendrogram
y_hc = hc.fit_predict(x)
print(y_hc)

#Visualizing the hierarcheal model
plt.scatter(x[y_hc == 0, 0], x[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(x[y_hc == 1, 0], x[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(x[y_hc == 2, 0], x[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(x[y_hc == 3, 0], x[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(x[y_hc == 4, 0], x[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

"""# **Association Rule Learning**

# ***Apriori***
"""

#Installing apriori packages from web as no library has it
!pip install apyori

# Story of people buying In a convinient store beer + diaper (that came from data)! How to get to know this
# A person who did this did something else
# person who watched movie 1 watched movie 2
# Apriori - 3 parts - support (frequency of item/ grp of items being brought), confidence(how often its brought together), lift(strength of the rule formed abt the association).

# steps
#1 minimum support and confidence
#2 take the subset having support more than min support
#3 take rules of this subset having confidence more than min confidence
#4 sort the rules by decreasing lift 

#eg recomandation systems have small part of it, business owner

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None) 

#apriori function has a different format than pandas, it takes in list. 
transactions = []

#populating the list
for i in range(0, 7501):
  transactions.append([str (dataset.values[i,j]) for j in range(0, 20)]) #list in a list

#training the apriori model on the dataset
#uploading the apriori function
from apyori import apriori

rules = apriori(transactions = transactions,min_support = 0.003 , min_confidence =  0.2 , min_lift = 3, min_length =2, max_length = 2) # 7 * 3 / 7501 = support

#visualizing the results
results = list (rules)
results

#putting results in pandas data frame

def inspect(results):
    lhs         = [tuple(result[2][0][0])[0] for result in results]
    rhs         = [tuple(result[2][0][1])[0] for result in results]
    supports    = [result[1] for result in results]
    confidences = [result[2][0][2] for result in results]
    lifts       = [result[2][0][3] for result in results]
    return list(zip(lhs, rhs, supports, confidences, lifts))
resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])

#displaying the result non sorted 
resultsinDataFrame

#Displaying the results sorted by descending lifts
resultsinDataFrame.nlargest(n = 10, columns = 'Lift')

"""# ***Eclat***"""

#movies, books recommendation etc, easier than apriori. here we are going using sets and not rules
# here we only have support = transaction containing x / no of transactions
!pip install apyori

# import libraries
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None) 

#apriori function has a different format than pandas, it takes in list. 
transactions = []

#populating the list
for i in range(0, 7501):
  transactions.append([str (dataset.values[i,j]) for j in range(0, 20)]) #list in a list

from apyori import apriori
rules = apriori(transactions = transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2, max_length = 2)

#visualizing the results
results = list (rules)
results

#putting results in pandas data frame

def inspect(results):
    lhs         = [tuple(result[2][0][0])[0] for result in results]
    rhs         = [tuple(result[2][0][1])[0] for result in results]
    supports    = [result[1] for result in results]
    return list(zip(lhs, rhs, supports))
resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])

#Displaying the results sorted by descending lifts
resultsinDataFrame.nlargest(n = 10, columns = 'Support')

"""# **Reinforcement**"""

# What is multi armed bandit problem ?
#upper confident bound

"""# ***Upper Confidence Bound***"""

#deterministic algo
#steps check about them
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Ads_CTR_Optimisation.csv')

#impliment of UCB
#there are 10 ads
import math
N = 10000 #try varing and check :) 

d = 10
ads_selected = []
numbers_of_selections = [0] * d
sums_of_rewards = [0] * d
total_reward = 0
for n in range(0, N):
  ad = 0
  max_upper_bound = 0
  for i in range(0, d):
    if (numbers_of_selections[i] > 0):
      average_reward = sums_of_rewards[i] / numbers_of_selections[i]
      delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])
      upper_bound = average_reward + delta_i
    else:
      upper_bound = 1e400
    if (upper_bound > max_upper_bound):
      max_upper_bound = upper_bound
      ad = i
  ads_selected.append(ad)
  numbers_of_selections[ad] = numbers_of_selections[ad] + 1
  reward = dataset.values[n, ad]
  sums_of_rewards[ad] = sums_of_rewards[ad] + reward
  total_reward = total_reward + reward

#Visualizing the results 
plt.hist(ads_selected) #histogram
plt.title('Histogram of ads selections')
plt.xlabel('Ads')
plt.ylabel('Number of times each ad was selected')
plt.show()

"""# ***Thompson Sampling***"""

#probablistic algorithm
#ucb VS thompson sampling
#ucb - deterministic algo, needs update every round(isnt easy for web sites where there are 10000/sec++ clicks etc), 
#thompson - probablistic algo, delayed feedback is ok,

#steps check about them
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Ads_CTR_Optimisation.csv')

#implimenting thompson sampling
import random
N = 10000 #total no of user , try 500 also
d = 10 #no of ads
ads_selected = [] #clicks
numbers_of_rewards_1 = [0] * d # each time a ad getting reward 1
numbers_of_rewards_0 = [0] * d # each time a ad getting reward 0
total_reward = 0 #total reward (accumulated reward)
for n in range(0, N):  # n = 10000 rounds who watch ad
  ad = 0   # ad which will be the index of the ad  
  max_random = 0 #maximum of the random
  for i in range(0, d):   #iterate from different ads , d  = no of ads
    random_beta = random.betavariate(numbers_of_rewards_1[i] + 1, numbers_of_rewards_0[i] + 1)  #random draw from beta variation
    if (random_beta > max_random): #max random will update in each iteration
      max_random = random_beta
      ad = i
  ads_selected.append(ad)
  reward = dataset.values[n, ad]
  if reward == 1:
    numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
  else:
    numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
  total_reward = total_reward + reward

#Visualizing the results 
plt.hist(ads_selected) #histogram
plt.title('Histogram of ads selections')
plt.xlabel('Ads')
plt.ylabel('Number of times each ad was selected')
plt.show()

"""# **Natural Language Processing**"""

#helps to built chat bot etc
#types of NLP, deep learning, DLNP, Seq2Seq
#classical 
#deep learning model
#if else rules, speech recognisition, Bag of words model, CNN for textual recognistion, 
#Bag of words - sos, eos, special words,  
#sentimental anaylsis example - tsv format, 1 means customer liked it.

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter = '\t', quoting = 3)  #can open tsv file using same command just add delimiter, quoting =3 means to ignore all ' in the file

#cleaning the texts
import re    #rep library used to simply but not steming
import nltk   # remove irrevelent words like the , a, he, she etc
nltk.download('stopwords')   #downloading all stop words
from nltk.corpus import stopwords   #importing the stop words
from nltk.stem.porter import PorterStemmer  #apply steming using, i.e is only the main content . i loved it -- loved i.e positive feedback
corpus = []  #all different reviews 
for i in range(0, 1000):  #cleaning process and after cleaning add it to corpus, 1000 reviews
  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # element not a letter replaced by space , reaching the text iloc also works, but review []  is also correct
  review = review.lower() # converted to lower 
  review = review.split() # prepare for steming, split the words into different words
  ps = PorterStemmer() #stemming class
  all_stopwords = stopwords.words('english') #
  all_stopwords.remove('not') #not include the not word in the stopword, as its a negative statement
  review = [ps.stem(word) for word in review if not word in set(all_stopwords)] #iterate to all the words, dont include stop words,  
  review = ' '.join(review) #join the words with space after removing everything unwanted
  corpus.append(review)


print(corpus)

#creating the bag of words model
from sklearn.feature_extraction.text import CountVectorizer 
cv = CountVectorizer(max_features = 1500) # max number of frequent repeating words 
X = cv.fit_transform(corpus).toarray() # fit and transforms the and puts in the array
y = dataset.iloc[:, -1].values #dependend variable i.e is the last column of our dataset

len(X[0])

#splitting the dataset into triaining 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

#training the naive base model on training set 
#try with other model 

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

#predicting the test set results
y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

#making the confusion matrix 
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""# **Deep Learning**"""

#Geofry Hinton - Father of DL (Works at GOOGLE)
#Mimicing how a brain works
#feature scaling is a must

"""# ***Artificial Neural Networks***"""

#Neuron - How to recreate a neuron?, weights, gets weighted sum when it pases to the next where activation function being applied
#Activation Function - different types of function - threshold, sigmoid(mostly used for probability), rectifier(most useful), hyperbolic tangeant
#How they work - activation function works only for a certain... attributes there, check an example.
#How they learn - perceptron, Update the weights checking the difference between y actual and y predicted and adjusted using back propogation
#Gradient Descent - minimize the errors using brute force(its one way) but when there are a lot of networks, local minimum and global min 
#Stochastic Gradient Descent -random, dosent require entire neural network to be correct, adjusting weights every time, avoids local minimum, its faster as it dosent need to load data 
#Batch Gradient Descent - Dterministic
#Back Propagation

#data - biz problem - risk of customer with highest risk of leaving

import numpy as np
import pandas as pd
import tensorflow as tf

tf.__version__

dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:-1].values
y = dataset.iloc[:, -1].values #check the features which will effect the data

print(X)
print(y)

#no missing data so we dont have to work on it
#encoding the categorical(string) data (country and gender)

#gender
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
X[:, 2] = le.fit_transform(X[:, 2])

#country
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
print(X)

#splitting the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

print(X_train)
print(X_test)

#feature scaling

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#initialising ann
ann = tf.keras.models.Sequential()

#you can experiment any number of layers, hidden layer should always have relu activation function

#adding input and 1st hidden layer
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

#adding 2nd hidden layer
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

#adding the output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) #probability of the output will also be given, 
#

#training the ann

#compiling the ann
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) #compare and update the weight using the optimiser
#as the output is binary(0/1) so binary_crossentropy else use categorical

#training the ann on training set
ann.fit(X_train, y_train, batch_size = 32, epochs = 100) #batch learning is more effcient, so that we can compare, epochs certain amount of epochs

#single prediction 
print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)
#format for the predict is 2d array , for female and germany we had to enter the encoding, 
#male =1, female =0
#change the scaling as feature scaling was applied 
# as we have applied sigmoid function the output is the probability is output 

#predicting the test results 
y_pred = ann.predict(X_test)
y_pred = (y_pred > 0.5)
#as the prediction is the probability to make it binary its >0.5
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

#confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred) 
#out of 100 86 were predicted to stay

"""# ***Convolutional Neural Networks***"""

#cnn- facebook tagging image by identifying pictures
# Yann Lecun - works at FB - student of Geofry Hinton 
#check paper on Jianxin
#convolutional operation - its combine integration of 2 functions. there is feature detector matrix and we compare. each match stride. Feature detector
#ReLU layer - we think that we might find something linear, relu removes all black. helps in introducing non linearlity , check paper by Kaiming He
#Pooling - mean pooling , max , sum pooling. -  basically reduce the number of parameters - paper by Dominik Scherer
#flattening - converting to `1d array by inputing to other layer
#full connection -
#soft max - making values between 0 - 1 . squashes it between 0 - 1. 
#cross entropy -
# Adit Deshpande - paper - 9 different CNN
# Rob DiPietro - paper , Peter Rolents

#Importing libraries  
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
tf.__version__

#preprosesing train data 
#cant run in google collab as the dataset is too big
#apply geometrical transformation to modify the images to avoid fit (over learn)

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)
#applys feature scaling (0-255) pixels and other transformation to prevent over fitting
#put the correct path
training_set = train_datagen.flow_from_directory('dataset/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

#connecting to the original training set, resizing to reduce the computation

#preprosing testing data
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('dataset/test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')

#binary out come(cat or dog), standard batch size = 32

#creating cnn
#initialising cnn
cnn = tf.keras.models.Sequential() #sequential layers

#convolution
cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) # con 2d class,no of filter/feature detoctors 
#kernal is the no of rows, input shape 64 64 3 , coloured images  

#pooling 
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
#pooled feature map , pool size = frame size 2*2 frame , strides = max 

#second convolution layer
cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))
#

#flattening
cnn.add(tf.keras.layers.Flatten())
#all the layers in 1 and flattened to one

#full connection
cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))

#output layer
cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
# 1 as its binary

#Training CNN

#compiling cnn
cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
#adam for stochastic gradient

#Training the CNN on the Training set and evaluating it on the Test set
cnn.fit(x = training_set, validation_data = test_set, epochs = 25)

#making a single prediction
import numpy as np
from keras.preprocessing import image
test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
training_set.class_indices
if result[0][0] == 1:
  prediction = 'dog'
else:
  prediction = 'cat'

print(prediction)

"""# **Dimensionality Reduction**

# ***Principal Component Analysis***
"""

#used in -  Noise filtering, Visualization, Feature Extraction, Stock Market Predictions, Gene Data Analysis

#identifys relation between variables (patterns)
#business case study - Customer recoomendation system on wine.

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Wine.csv') 
x = dataset.iloc[:,:-1].values
y =  dataset.iloc[:,-1].values

#splitting data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

#feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train[:,3:] =sc.fit_transform(x_train[:,3:])
x_test[:,3:] = sc.fit_transform(x_test[:,3:])

#Applying PCA
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test) #just transfomr to avoid information lekage

#Training with Logistic Regression
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)

#Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Visualizing Train Set
from matplotlib.colors import ListedColormap
X_set, y_set = x_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

#Visualizing Test Set
from matplotlib.colors import ListedColormap
X_set, y_set = x_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

"""# ***Linear Discriminant Analysis***"""

#Common Dimension Reduction Technique
#used in preprocessing stage of data set with a goal of reducing onto a lower dimensional space
#How LDA and PCA differs - LDA maximises the seperation bewteen multiple classes

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Wine.csv') 
x = dataset.iloc[:,:-1].values
y =  dataset.iloc[:,-1].values

#splitting data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

#feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train[:,3:] =sc.fit_transform(x_train[:,3:])
x_test[:,3:] = sc.fit_transform(x_test[:,3:])

#Applying LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components = 2)
x_train = lda.fit_transform(x_train, y_train) #both x_train and the dependant variable y_train
x_test = lda.transform(x_test)

#Training with Logistic Regression
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)

#Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Visualizing Train Set
from matplotlib.colors import ListedColormap
X_set, y_set = x_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

#Visualizing Test Set
from matplotlib.colors import ListedColormap
X_set, y_set = x_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

"""# ***Kernal PCA***"""

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Wine.csv') 
x = dataset.iloc[:,:-1].values
y =  dataset.iloc[:,-1].values

#splitting data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

#feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

#Applying Kernal PCA
from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components = 2, kernel = 'rbf')
x_train = kpca.fit_transform(x_train)
x_test = kpca.transform(x_test)

#Training with Logistic Regression
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train, y_train)

#Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Visualizing Train Set
from matplotlib.colors import ListedColormap
X_set, y_set = x_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

#Visualizing Test Set
from matplotlib.colors import ListedColormap
X_set, y_set = x_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()

"""# **Model Selection AND Boosting**

# ***K-Fold Cross Validation***
"""

#Measures perfromance

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

#Training Kernal SVM
from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(x_train, y_train)

#Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Apply K fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('K-Fold Cross Validation')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('K-Fold Cross Validation')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***Grid Search***"""

#find best version of model with the given variables

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd

#open the document
dataset = pd.read_csv('Social_Network_Ads.csv') 

#leave the 1st column
x = dataset.iloc[:,:-1].values #didnt include the 1st column
y =  dataset.iloc[:,-1].values
print(x)
print(y)

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)


#feature scaling -  It isnt really needed, But applying it will increase training performance improving final prediction 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

#last column is already in terms of zeros and ones 
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

#Training Kernal SVM
from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(x_train, y_train)

#Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Applying K-Fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

#Applying Grid Search to find the best model and the best parameters
from sklearn.model_selection import GridSearchCV
#combinations of parameters you want to test
parameters = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},
              {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
grid_search = GridSearchCV(estimator = classifier,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search.fit(x_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

#visualizing training set

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Grid Search ')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

#visualizing test set result
from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_train), y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),
                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Grid Search')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# ***XGBoost***"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#open the document
#Dataset on the characteristics of a tumor
dataset = pd.read_csv('Data1.csv')

#leave the 1st column
x = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

#splitting the dataset

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)

#Training on XGBoost
from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(x_train, y_train)

#Making Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

#Applying K-Fold Cross Validation
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))